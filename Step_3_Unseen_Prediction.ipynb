{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Unseen Data Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will use logistic regression because it outperformed all the other methods. For some reason the csv file was not opened by the computer we are using, therefore we opened it with another one and saved the dataframe in a pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/INTRANET/mpietrob/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Dictionary(25989 unique tokens: ['hard', 'believe', 'memory', 'tree', 'came']...)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seed = 123\n",
    "\n",
    "df_complete = pd.read_json(path_or_buf = 'data/reviews_Digital_Music_5.json', lines=True)\n",
    "df1 = pd.DataFrame(df_complete[['asin', 'overall','reviewText']])\n",
    "del df_complete\n",
    "\n",
    "df_complete = df = pd.read_pickle('amazon_step2_unseen.pkl')\n",
    "df2 = pd.DataFrame(df_complete[['asin','reviewText']])\n",
    "del df_complete\n",
    "\n",
    "\n",
    "#JOIN TRAIN AND TEST SET TO PRE-PROCESS THEM TOGETHER\n",
    "df = pd.concat([df1.ix[:,[0,2]], df2])\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df_refined=[]\n",
    "for item in df.ix[:,2]:\n",
    "    df_refined.append (item.replace('\\r',' ').replace('/n',' ').replace('.',' ')\\\n",
    "                           .replace(',',' ').replace('(',' ').replace(')',' ')\\\n",
    "                           .replace(\"'s\",' ').replace('\"',' ').replace('!',' ')\\\n",
    "                           .replace('?',' ').replace(\"'\",' ').replace('>',' ')\\\n",
    "                           .replace('$',' ').replace('-',' ').replace(';',' ')\\\n",
    "                           .replace(':',' ').replace('/',' ').replace('#',' '))\n",
    "    \n",
    "from gensim import corpora, models, matutils\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "\n",
    "download('wordnet')\n",
    "\n",
    "tester = 1\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "documents = df_refined\n",
    "\n",
    "# removing stopwords\n",
    "documents_no_stop = [[word for word in document.lower().split() if word not in STOPWORDS]\n",
    "         for document in documents]\n",
    "\n",
    "del documents\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "threshold = 1 # frequency threshold\n",
    "frequency = defaultdict(int)\n",
    "for text in documents_no_stop:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "documents_no_stop_no_unique = [[token for token in text if frequency[token] > threshold] \n",
    "                               for text in documents_no_stop]\n",
    "\n",
    "del documents_no_stop\n",
    "\n",
    "# remove all numerics and tokens with numbers\n",
    "import re\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "documents_no_stop_no_unique_no_numeric = [[token for token in text if not (hasNumbers(token)) ] \n",
    "                                          for text in documents_no_stop_no_unique]\n",
    "\n",
    "del documents_no_stop_no_unique\n",
    "\n",
    "# lemmattizing tokens (better than stemming by taking word context into account)\n",
    "documents_no_stop_no_unique_no_numeric_lemmatize = [[lemmatizer.lemmatize(token) for token in text] \n",
    "                                                    for text in documents_no_stop_no_unique_no_numeric]\n",
    "\n",
    "del documents_no_stop_no_unique_no_numeric\n",
    "\n",
    "import enchant\n",
    "eng_dic = enchant.Dict(\"en_US\")\n",
    "\n",
    "# remove non-english words\n",
    "documents_no_stop_no_unique_no_numeric_lemmatize_english = [[token for token in text if (eng_dic.check(token)) ] \n",
    "                                                            for text in documents_no_stop_no_unique_no_numeric_lemmatize]\n",
    "\n",
    "del documents_no_stop_no_unique_no_numeric_lemmatize\n",
    "\n",
    "# create ready corpus\n",
    "ready_corpus = documents_no_stop_no_unique_no_numeric_lemmatize_english\n",
    "\n",
    "# build the dictionary and store it to disc for future use\n",
    "dictionary = corpora.Dictionary(ready_corpus)\n",
    "print(dictionary)\n",
    "\n",
    "# convert the corpus into bag of words \n",
    "corpus_bow = [dictionary.doc2bow(comment) for comment in ready_corpus]\n",
    "\n",
    "tfidf_transformer = models.TfidfModel(corpus_bow, normalize=True)\n",
    "\n",
    "# apply tfidf transformation to the bow corpus\n",
    "corpus_tfidf = tfidf_transformer [corpus_bow]\n",
    "\n",
    "# convert to a sparse and compatible format for dimensionality reduction using sklearn\n",
    "sparse_corpus_tfidf = matutils.corpus2csc(corpus_tfidf)\n",
    "sparse_corpus_tfidf_transpose = sparse_corpus_tfidf.transpose()\n",
    "\n",
    "X_train = sparse_corpus_tfidf_transpose[:df1.shape[0],:]\n",
    "X_test = sparse_corpus_tfidf_transpose[df1.shape[0]:,:]\n",
    "y_train = df1.ix[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(C=3.6)\n",
    "\n",
    "print('Fitting Logistic Regression')\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "y_cap = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#SAVE FILE AS CSV\n",
    "data = np.column_stack((df2['asin'],y_cap))\n",
    "np.savetxt('predictions/step3.csv',data, fmt=\"%s\", delimiter = \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
