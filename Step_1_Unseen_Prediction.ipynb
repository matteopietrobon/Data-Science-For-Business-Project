{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Unseen Data Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do such prediction we use the results from previous points. We will therefore use a Logistic Regression model for two reasons:<br>\n",
    "1) It has shown the best cross-validated performance among all models<br>\n",
    "2) With the computational limits we have it has proven to be the more flexible and fast model. Moreover the logistic regression model allows us to ignore PCA decomposition (that we have shown in preprocessing phase to be detrimental in terms of model performance) and use the entire dataset. The other models didn't manage to run with such a big amount of data.<br>\n",
    "\n",
    "<br>\n",
    "We start with the usual preprocessing, but this time we keep train and \"unseen\" set together. We will separate them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/INTRANET/mpietrob/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Dictionary(29118 unique tokens: ['caliber', 'ammo', 'largely', 'sum', 'getting']...)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seed = 123\n",
    "\n",
    "df_complete = pd.read_json('data/amazon_step1.json', lines=True)\n",
    "df1 = pd.DataFrame(df_complete[['asin', 'category','reviewText']])\n",
    "del df_complete\n",
    "\n",
    "# Group comments by product and category assigned\n",
    "united = df1.groupby(['asin', 'category'])['reviewText'].apply(' '.join).reset_index()\n",
    "\n",
    "old = 0\n",
    "\n",
    "product_numbers = []\n",
    "\n",
    "# create the list of the products with more than one category assigned\n",
    "for _,row in united.iterrows():\n",
    "    \n",
    "    if(old == row['asin']):\n",
    "        \n",
    "        product_numbers.append(row['asin'])\n",
    "        \n",
    "    old = row['asin']\n",
    "\n",
    "    \n",
    "indexes=[]\n",
    "\n",
    "# find what are the comments related to the products found above\n",
    "for i,df_row in df1.iterrows():\n",
    "    \n",
    "    if df_row['asin'] in product_numbers:\n",
    "        \n",
    "        indexes.append(i)\n",
    "\n",
    "# drop the ambiguous observations\n",
    "df1.drop(df1.index[[indexes]], inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "df_complete = pd.read_csv('data/amazon_step1_unseen.csv')\n",
    "df2 = pd.DataFrame(df_complete[['asin','reviewText']])\n",
    "del df_complete\n",
    "\n",
    "\n",
    "#JOIN TRAIN AND TEST SET TO PRE-PROCESS THEM TOGETHER\n",
    "df = pd.concat([df1.ix[:,[0,2]], df2])\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "drop_indexes = []\n",
    "for i in range(1,df.shape[0]):\n",
    "    if(isinstance(df.ix[i,2],str) == False):\n",
    "    \n",
    "        drop_indexes.append(i)\n",
    "        \n",
    "#THESE ELEMENTS are NaNs I drop them\n",
    "dff = df.drop(df.index[drop_indexes])\n",
    "\n",
    "\n",
    "# All the special characters were removed from the sample\n",
    "df_refined=[]\n",
    "for item in dff.ix[:,2]:\n",
    "    df_refined.append (item.replace('\\r',' ').replace('/n',' ').replace('.',' ')\\\n",
    "                           .replace(',',' ').replace('(',' ').replace(')',' ')\\\n",
    "                           .replace(\"'s\",' ').replace('\"',' ').replace('!',' ')\\\n",
    "                           .replace('?',' ').replace(\"'\",' ').replace('>',' ')\\\n",
    "                           .replace('$',' ').replace('-',' ').replace(';',' ')\\\n",
    "                           .replace(':',' ').replace('/',' ').replace('#',' '))\n",
    "    \n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "documents = df_refined\n",
    "\n",
    "# remove the stopwords\n",
    "documents_no_stop = [[word for word in document.lower().split() if word not in STOPWORDS]\n",
    "         for document in documents]\n",
    "\n",
    "del documents\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "threshold = 1 # frequency threshold\n",
    "frequency = defaultdict(int)\n",
    "for text in documents_no_stop:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "documents_no_stop_no_unique = [[token for token in text if frequency[token] > threshold] \n",
    "                               for text in documents_no_stop]\n",
    "\n",
    "del documents_no_stop\n",
    "\n",
    "# remove all numerics and tokens with numbers\n",
    "import re\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "documents_no_stop_no_unique_no_numeric = [[token for token in text if not (hasNumbers(token)) ] \n",
    "                                          for text in documents_no_stop_no_unique]\n",
    "\n",
    "del documents_no_stop_no_unique\n",
    "\n",
    "# lemmattizing tokens (better than stemming by taking word context into account)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "\n",
    "download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "documents_no_stop_no_unique_no_numeric_lemmatize = [[lemmatizer.lemmatize(token) for token in text] \n",
    "                                                    for text in documents_no_stop_no_unique_no_numeric]\n",
    "\n",
    "# remove non-english words\n",
    "import enchant\n",
    "eng_dic = enchant.Dict(\"en_US\")\n",
    "\n",
    "documents_no_stop_no_unique_no_numeric_lemmatize_english = [[token for token in text if (eng_dic.check(token)) ] \n",
    "                                                            for text in documents_no_stop_no_unique_no_numeric_lemmatize]\n",
    "\n",
    "del documents_no_stop_no_unique_no_numeric_lemmatize\n",
    "\n",
    "# create ready corpus\n",
    "ready_corpus = documents_no_stop_no_unique_no_numeric_lemmatize_english\n",
    "\n",
    "# build the dictionary and store it to disc for future use\n",
    "dictionary = corpora.Dictionary(ready_corpus)\n",
    "\n",
    "# convert the corpus into bag of words \n",
    "from gensim import models, corpora, matutils\n",
    "dictionary = corpora.Dictionary(ready_corpus)\n",
    "print(dictionary)\n",
    "\n",
    "corpus_bow = [dictionary.doc2bow(comment) for comment in ready_corpus]\n",
    "\n",
    "tfidf_transformer = models.TfidfModel(corpus_bow, normalize=True)\n",
    "\n",
    "# apply tfidf transformation to the bow corpus\n",
    "corpus_tfidf = tfidf_transformer [corpus_bow]\n",
    "\n",
    "# convert to a sparse and compatible format\n",
    "sparse_corpus_tfidf = matutils.corpus2csc(corpus_tfidf)\n",
    "sparse_corpus_tfidf_transpose = sparse_corpus_tfidf.transpose()\n",
    "\n",
    "\n",
    "X_train = sparse_corpus_tfidf_transpose[:df1.shape[0],:]\n",
    "X_test = sparse_corpus_tfidf_transpose[df1.shape[0]:,:]\n",
    "y_train = df1.ix[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we only have to run our preferred model, let it fit the train set and predict the 'unseen' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(C=1.0)\n",
    "\n",
    "print('Fitting Logistic Regression')\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "y_cap = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we join the prediction 'y_cap' with the test data and store it in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REINSERT NANS AND SAVE FILE AS CSV\n",
    "import numpy as np\n",
    "\n",
    "for i in range(0,len(drop_indexes)):\n",
    "    \n",
    "    drop_indexes[i] = drop_indexes[i] - df1.shape[0]\n",
    "\n",
    "for i in drop_indexes:\n",
    "    \n",
    "    #In absence of text we randomly guess the \n",
    "    #most frequent category in the test set\n",
    "    y_cap = np.insert(y_cap,i,'Grocery_and_Gourmet_Food')\n",
    "\n",
    "data = np.column_stack((df2['asin'],y_cap))\n",
    "np.savetxt('predictions/step1.csv',data, fmt=\"%s\", delimiter = \",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
